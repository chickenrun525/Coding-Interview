Design Uber

Write intense workload, not read intense workload

Core functions:
1. customer and drivers matching
2. mapping

Trip storage (low latency for rating or some other functions): one copy stored at a close date center, the other stored somewhere else
for backup, maybe a third stored far away.
Periodically send trip to a data warehouse (Hadoop) for analytics, OLAP, this no need for low latency
Caching functions, speeds up read for static data, like events, places of interest, not drivers' positions, dynamic data
Uber uses Kafka to ingest logging messages in real-time, different Kafka clusters periodically communicate to Hadoop
Uber uses SOA (check what is SOA), provisioning (Terraform), containers (Docker, Mesos) running as a separate

Network routing: request needs stateless communication between server and client
Hailstorm helps resiliency checking, see the relationship between different servers and different systems.

Uber is graph problem.
TSP problem (NP-hard) traveling salesperson problem
ETA: estimated time arrival
intersections are nodes, edges are connections
These are static data, consider traffic data, weather information
Rely on historical data, from database, search for a similar situation in the past for ETA calculation, needs efficient lookup
For a new city where there is not much historical data:
either use A* or Djikstra algorithm in real time, split the city into smaller parts, then use graph algorithm

Some other problems like rating a driver, sth.....

Uber technology: RingPop, TChannel, Google S2, Riak

4S procedures:
1. Logical design:
Scenario, Service, Storage
2. Infrastructure design:
Scale

@Scenario: drivers update location, rider requests Uber, Uber gives a few drivers nearby, rider chooses a driver, driver accepts/denies rider
rider can cancel the request, driver picks up the rider, driver drops the rider

Data estimation: driver QPS: 0.5 million at the same time, every 5 seconds update once, 500k/5 = 100k, peak = average*3 = 300k
rider much smaller than drivers

data amount = 100k*86400*100b = 864GBï¼Œaround 1TB

@Service:
matching service (dispatch service); mapping service(Geo service)
what kind of data is needed? Stores trip information, stores location information

public class trip { # to track the information of a trip
    tripid, driverid, riderid, start position, end position, time, status
}

public class Location { # to match a driver with a rider
    driverid, position, updatetime
}

public class driver { # to check if a driver is available
    driver id;
    position;
    status;
}

Geohash or Google s2 to locate precisely the location of the driver

@Storage: SQL- create index on geohash and like query
          NoSQL-Casandra: geohash: column key and range query
          NoSQL-Redis/Memcached: store the drive position in different level of distances based on the GEO hash Key

Compare each kind of choice advantages and disadvantages
generate a location table when a rider requests uber
key: rider geohash, value: driver id

After dispatching services, return driver position
key: driver id
value: driver position, updatetime, trip_id

Question: Driver table and location table are stored by Nosql+redis, trip table and user table are in SQL

GEO-fence: https://eng.uber.com/go-geofence/

@Scalibity: DB sharding based on city
            replica by redis (master + slave)
            Because of sharding, not much network flow now, so Riak/Cassandra + NoSQL is possible

Some potential challenge: no enough rider/driver matching. Safety protection.
Check bitTiger video for how to efficiently communicate P2P?

Some other function like push notification for rider request and rider receipt
